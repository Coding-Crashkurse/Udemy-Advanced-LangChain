{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app_dir = os.path.join(os.getcwd(), \"app\")\n",
    "load_dotenv(os.path.join(app_dir, \".env\"))\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=120,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "model = ChatOpenAI()\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import re\n",
    "\n",
    "query = \"Who owns the restaurant?\"\n",
    "\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative question like this:\n",
    "    <<question1>>\n",
    "    <<question2>>\n",
    "    Only provide the query, no numbering.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def split_and_clean_text(input_text):\n",
    "    return [item for item in re.split(r\"<<|>>\", input_text) if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "rephrase_chain = (\n",
    "    QUERY_PROMPT | model | StrOutputParser() | RunnableLambda(split_and_clean_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_questions = rephrase_chain.invoke(\"Who is the owner of the restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [retriever.get_relevant_documents(q) for q in list_of_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_unique_documents(documents):\n",
    "    flattened_docs = [doc for sublist in documents for doc in sublist]\n",
    "\n",
    "    unique_docs = []\n",
    "    unique_contents = set()\n",
    "    for doc in flattened_docs:\n",
    "        if doc.page_content not in unique_contents:\n",
    "            unique_docs.append(doc)\n",
    "            unique_contents.add(doc.page_content)\n",
    "\n",
    "    return unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_and_unique_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDE_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five hypothetical answers to the user's query. These answers should offer diverse perspectives or interpretations, aiding in a comprehensive understanding of the query. Present the hypothetical answers as follows:\n",
    "\n",
    "    Hypothetical Answer 1: <<Answer considering a specific perspective>>\n",
    "    Hypothetical Answer 2: <<Answer from a different angle>>\n",
    "    Hypothetical Answer 3: <<Answer exploring an alternative possibility>>\n",
    "    Hypothetical Answer 4: <<Answer providing a contrasting viewpoint>>\n",
    "    Hypothetical Answer 5: <<Answer that includes a unique insight>>\n",
    "\n",
    "    Note: Present only the hypothetical answers, without numbering, to provide a range of potential interpretations or solutions related to the query.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_chain = (\n",
    "    HYDE_PROMPT | model | StrOutputParser() | RunnableLambda(split_and_clean_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_questions = hyde_chain.invoke(\"Who is the owner of the restaurant\")\n",
    "list_of_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [retriever.get_relevant_documents(q) for q in list_of_questions]\n",
    "flatten_and_unique_documents(documents=docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
