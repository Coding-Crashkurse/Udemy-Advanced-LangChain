{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app_dir = os.path.join(os.getcwd(), \"app\")\n",
    "load_dotenv(os.path.join(app_dir, \".env\"))\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = InMemoryStore()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "#parent_splitter = RecursiveCharacterTextSplitter(chunk_size=600)\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    #parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(docstore.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"who is the owner?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom Store with PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str] = Field(None)\n",
    "    page_content: Optional[str] = Field(None)\n",
    "    metadata: dict = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class SQLDocument(Base):\n",
    "    __tablename__ = \"docstore\"\n",
    "    key = Column(String, primary_key=True)\n",
    "    value = Column(JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key='{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Generic, Iterator, Sequence, TypeVar\n",
    "from langchain.schema import Document\n",
    "from langchain_core.stores import BaseStore\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "D = TypeVar(\"D\", bound=Document)\n",
    "\n",
    "class PostgresStore(BaseStore[str, DocumentModel], Generic[D]):\n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine = create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session = scoped_session(sessionmaker(bind=self.engine))\n",
    "\n",
    "    def serialize_document(self, doc: Document) -> dict:\n",
    "        return {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "\n",
    "    def deserialize_document(self, value: dict) -> Document:\n",
    "        return Document(page_content=value.get(\"page_content\", \"\"), metadata=value.get(\"metadata\", {}))\n",
    "\n",
    "\n",
    "    def mget(self, keys: Sequence[str]) -> list[Document]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                sql_documents = session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).all()\n",
    "                return [self.deserialize_document(sql_doc.value) for sql_doc in sql_documents]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mget: {e}\")\n",
    "                session.rollback()\n",
    "                return []\n",
    "\n",
    "\n",
    "    def mset(self, key_value_pairs: Sequence[tuple[str, Document]]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                serialized_docs = []\n",
    "                for key, document in key_value_pairs:\n",
    "                    serialized_doc = self.serialize_document(document)\n",
    "                    serialized_docs.append((key, serialized_doc))\n",
    "\n",
    "                documents_to_update = [SQLDocument(key=key, value=value) for key, value in serialized_docs]\n",
    "                session.bulk_save_objects(documents_to_update, update_changed_only=True)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mset: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "\n",
    "    def mdelete(self, keys: Sequence[str]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).delete(synchronize_session=False)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mdelete: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    def yield_keys(self, *, prefix: Optional[str] = None) -> Iterator[str]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                query = session.query(SQLDocument.key)\n",
    "                if prefix:\n",
    "                    query = query.filter(SQLDocument.key.like(f\"{prefix}%\"))\n",
    "                for key in query:\n",
    "                    yield key[0]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in yield_keys: {e}\")\n",
    "                session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\AdvancedLangChain\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:289: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg2://admin:admin@localhost:5432/vectordb\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=store,\n",
    "    docstore=PostgresStore(connection_string=DATABASE_URL),\n",
    "    child_splitter=child_splitter,\n",
    "    #parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"who is the owner?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
