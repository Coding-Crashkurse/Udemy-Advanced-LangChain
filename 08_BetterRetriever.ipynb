{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app_dir = os.path.join(os.getcwd(), \"app\")\n",
    "load_dotenv(os.path.join(app_dir, \".env\"))\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = InMemoryStore()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=600)\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(docstore.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"who is the owner?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom Store with PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str] = Field(None)\n",
    "    page_content: Optional[str] = Field(None)\n",
    "    metadata: dict = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class SQLDocument(Base):\n",
    "    __tablename__ = \"docstore\"\n",
    "    key = Column(String, primary_key=True)\n",
    "    value = Column(JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key='{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Generic, Iterator, Sequence, TypeVar\n",
    "from langchain.schema import Document\n",
    "from langchain_core.stores import BaseStore\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "D = TypeVar(\"D\", bound=Document)\n",
    "\n",
    "\n",
    "class PostgresStore(BaseStore[str, DocumentModel], Generic[D]):\n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine = create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session = scoped_session(sessionmaker(bind=self.engine))\n",
    "\n",
    "    def serialize_document(self, doc: Document) -> dict:\n",
    "        return {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "\n",
    "    def deserialize_document(self, value: dict) -> Document:\n",
    "        return Document(\n",
    "            page_content=value.get(\"page_content\", \"\"),\n",
    "            metadata=value.get(\"metadata\", {}),\n",
    "        )\n",
    "\n",
    "    def mget(self, keys: Sequence[str]) -> list[Document]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                sql_documents = (\n",
    "                    session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).all()\n",
    "                )\n",
    "                return [\n",
    "                    self.deserialize_document(sql_doc.value)\n",
    "                    for sql_doc in sql_documents\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mget: {e}\")\n",
    "                session.rollback()\n",
    "                return []\n",
    "\n",
    "    def mset(self, key_value_pairs: Sequence[tuple[str, Document]]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                serialized_docs = []\n",
    "                for key, document in key_value_pairs:\n",
    "                    serialized_doc = self.serialize_document(document)\n",
    "                    serialized_docs.append((key, serialized_doc))\n",
    "\n",
    "                documents_to_update = [\n",
    "                    SQLDocument(key=key, value=value) for key, value in serialized_docs\n",
    "                ]\n",
    "                session.bulk_save_objects(documents_to_update, update_changed_only=True)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mset: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    def mdelete(self, keys: Sequence[str]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).delete(\n",
    "                    synchronize_session=False\n",
    "                )\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mdelete: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    def yield_keys(self) -> Iterator[str]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                query = session.query(SQLDocument.key)\n",
    "                for key in query:\n",
    "                    yield key[0]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in yield_keys: {e}\")\n",
    "                session.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg://admin:admin@localhost:5432/vectordb\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=store,\n",
    "    docstore=PostgresStore(connection_string=DATABASE_URL),\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"who is the owner?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
