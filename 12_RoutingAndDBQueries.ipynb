{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app_dir = os.path.join(os.getcwd(), \"app\")\n",
    "load_dotenv(os.path.join(app_dir, \".env\"))\n",
    "\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg://admin:admin@localhost:5432/vectordb\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "loader1 = TextLoader(\"./data/restaurant.txt\")\n",
    "loader2 = TextLoader(\"./data/founder.txt\")\n",
    "\n",
    "docs2 = loader1.load()\n",
    "docs1 = loader2.load()\n",
    "docs = docs1 + docs2\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
    "chunks = splitter.split_documents(docs)\n",
    "store.add_documents(chunks)\n",
    "retriever = store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke({\"question\": \"Who is the owner of the restaurant?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "CONNECTION_STRING = (\n",
    "    \"postgresql+psycopg://readonlyuser:readonlypassword@localhost:5432/vectordb\"\n",
    ")\n",
    "\n",
    "db = SQLDatabase.from_uri(CONNECTION_STRING)\n",
    "\n",
    "\n",
    "def get_schema(_):\n",
    "    schema = db.get_table_info()\n",
    "    return schema\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_schema(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def get_schema(_):\n",
    "    engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "    inspector = inspect(engine)\n",
    "    columns = inspector.get_columns(\"products\")\n",
    "\n",
    "    column_data = [\n",
    "        {\n",
    "            \"Column Name\": col[\"name\"],\n",
    "            \"Data Type\": str(col[\"type\"]),\n",
    "            \"Nullable\": \"Yes\" if col[\"nullable\"] else \"No\",\n",
    "            \"Default\": col[\"default\"] if col[\"default\"] else \"None\",\n",
    "            \"Autoincrement\": \"Yes\" if col[\"autoincrement\"] else \"No\",\n",
    "        }\n",
    "        for col in columns\n",
    "    ]\n",
    "    schema_output = tabulate(column_data, headers=\"keys\", tablefmt=\"grid\")\n",
    "    formatted_schema = f\"Schema for 'PRODUCTS' table:\\n{schema_output}\"\n",
    "\n",
    "    return formatted_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_schema(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sql_response.invoke({\"question\": \"Whats the most expensive desert you offer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def debug(input):\n",
    "    print(\"SQL Output: \", input[\"query\"])\n",
    "    return input\n",
    "\n",
    "\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_response).assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x: run_query(x[\"query\"]),\n",
    "    )\n",
    "    | RunnableLambda(debug)\n",
    "    | prompt_response\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_chain.invoke({\"question\": \"Whats the most expensive dessert you offer?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can go wrong? Users could run potential malicious queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_chain.invoke({\"question\": \"Drop all products from the products table\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import ProgrammingError\n",
    "from psycopg2.errors import InsufficientPrivilege\n",
    "\n",
    "try:\n",
    "    result = sql_chain.invoke({\"question\": \"Drop all products from the products table\"})\n",
    "except ProgrammingError as pe:\n",
    "    if isinstance(pe.orig, InsufficientPrivilege):\n",
    "        result = \"Haha nice try! Got ya!\"\n",
    "    else:\n",
    "        result = \"An unexpected error occurred\"\n",
    "except Exception as e:\n",
    "    result = \"An unexpected error occurred\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "classification_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are good at classifying a question.\n",
    "    Given the user question below, classify it as either being about `Database`, `Chat` or 'Offtopic'.\n",
    "\n",
    "    <If the question is about products of the restaurant OR ordering food classify the question as 'Database'>\n",
    "    <If the question is about restaurant related topics like opening hours and similar topics, classify it as 'Chat'>\n",
    "    <If the question is about whether, football or anything not related to the restaurant or\n",
    "    products, classify the question as 'offtopic'>\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Classification:\"\"\"\n",
    ")\n",
    "\n",
    "classification_chain = classification_template | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_chain.invoke({\"question\": \"How is the wheather?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"database\" in info[\"topic\"].lower():\n",
    "        return sql_chain\n",
    "    elif \"chat\" in info[\"topic\"].lower():\n",
    "        return rag_chain\n",
    "    else:\n",
    "        return \"I am sorry, I am not allowed to answer about this topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "full_chain = RunnableParallel(\n",
    "    {\n",
    "        \"topic\": classification_chain,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    ") | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"Whats the most expensive dessert you offer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"How will the wheater be tomorrow?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"Who is the owner of the restaurant?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
